{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **DQN Implementation with pytorch**"
      ],
      "metadata": {
        "id": "8wrT7ENXDwXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/172566"
      ],
      "metadata": {
        "id": "eHviJ7J9aMqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GnMnz142V0Kb"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import gymnasium as gym\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Environment\n",
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "mPnCoO_5a4ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.action_space.n)\n",
        "print(env.observation_space.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR2fgW5sjXgy",
        "outputId": "24f14d05-0c3c-42f2-af74-85aaab994802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(env.action_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "68kGTFjHpCXL",
        "outputId": "bc2a3021-7d0f-4abf-fc8b-35eb3ba6b3ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gymnasium.spaces.discrete.Discrete"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>gymnasium.spaces.discrete.Discrete</b><br/>def __init__(n: int | np.integer[Any], seed: int | np.random.Generator | None=None, start: int | np.integer[Any]=0)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/discrete.py</a>A space consisting of finitely many elements.\n",
              "\n",
              "This class represents a finite subset of integers, more specifically a set of the form :math:`\\{ a, a+1, \\dots, a+n-1 \\}`.\n",
              "\n",
              "Example:\n",
              "    &gt;&gt;&gt; from gymnasium.spaces import Discrete\n",
              "    &gt;&gt;&gt; observation_space = Discrete(2, seed=42) # {0, 1}\n",
              "    &gt;&gt;&gt; observation_space.sample()\n",
              "    np.int64(0)\n",
              "    &gt;&gt;&gt; observation_space = Discrete(3, start=-1, seed=42)  # {-1, 0, 1}\n",
              "    &gt;&gt;&gt; observation_space.sample()\n",
              "    np.int64(-1)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 12);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQvZ2dDus74d",
        "outputId": "e6797ef3-ee93-4c02-c014-dbeb6f1ebe6c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk5fuvzK0n7S",
        "outputId": "542bffe2-35bd-46ee-de51-34de17e2619c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.03877578,  0.04721831,  0.01122632, -0.00451282], dtype=float32),\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(1) # cartpole에서 reward ->  1 per each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjkb2aP33QEo",
        "outputId": "463de72b-96c2-4bc5-bf08-673aa0597685"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.03783142,  0.24217747,  0.01113606, -0.29363266], dtype=float32),\n",
              " 1.0,\n",
              " False,\n",
              " False,\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of Q-network\n",
        "class Qnet(nn.Module):\n",
        "  def __init__(self, state_size, action_size, hidden_size = 256):\n",
        "    super(Qnet, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(state_size, hidden_size)     # Input : 4 parameters -> cart position, cart velocity, pole angle, pole velocity at tip\n",
        "    self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.layer3 = nn.Linear(hidden_size, action_size)   # Output : left or right\n",
        "    self.act = nn.GELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.act(self.layer1(x))\n",
        "    x = self.act(self.layer2(x))\n",
        "    return self.layer3(x)"
      ],
      "metadata": {
        "id": "KPZ1OkhSb1-y"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent():\n",
        "  def __init__(self, env, buffer_limit = 2000, batch_size = 32, gamma = 0.99, lr = 0.001):\n",
        "    self.env = env\n",
        "    self.buffer_limit = buffer_limit\n",
        "    self.replay_buffer = deque(maxlen = self.buffer_limit) # Replay Buffer for Replay Memory\n",
        "    self.q_network = Qnet(\n",
        "        state_size = self.env.observation_space.shape[0],\n",
        "        action_size = self.env.action_space.n,\n",
        "        hidden_size = 128\n",
        "        )\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "    self.optimizer = optim.Adam(\n",
        "        params = self.q_network.parameters(),\n",
        "        lr = lr\n",
        "    )\n",
        "\n",
        "# Exploration - Exploitation trade-off\n",
        "  def get_action(self, state, epsilon) -> int:\n",
        "    if np.random.rand() <= epsilon: # -> Exploration\n",
        "      return self.env.action_space.sample()\n",
        "\n",
        "    else: # -> Exploitation\n",
        "      q_value = self.q_network(torch.from_numpy(state).float().unsqueeze(0))[0]\n",
        "      return torch.argmax(q_value).item()\n",
        "\n",
        "  def append_sample(self, state, action, reward, next_state, done):\n",
        "    self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def train_step(self):\n",
        "    if len(self.replay_buffer) < self.batch_size:\n",
        "      return\n",
        "\n",
        "    mini_batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "    states, actions, rewards , next_states, dones = zip(*mini_batch)\n",
        "\n",
        "    states = np.array(states, dtype=np.float32) # UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor\n",
        "    actions = np.array(actions, dtype=np.int64)\n",
        "    rewards = np.array(rewards, dtype=np.float32)\n",
        "    next_states = np.array(next_states, dtype=np.float32)\n",
        "    dones = np.array(dones, dtype=np.float32)\n",
        "\n",
        "    states = torch.tensor(states, dtype = torch.float32)\n",
        "    actions = torch.tensor(actions, dtype = torch.int64)\n",
        "    rewards = torch.tensor(rewards, dtype = torch.float32)\n",
        "    next_states = torch.tensor(next_states, dtype = torch.float32)\n",
        "    dones = torch.tensor(dones, dtype = torch.float32)\n",
        "\n",
        "    curr_Qs = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "    next_Qs = self.q_network(next_states).max(1)[0].detach() # .max() returns ((values), (indicies))\n",
        "    target_Qs = rewards + self.gamma * next_Qs * (1 - dones)\n",
        "\n",
        "    loss = F.mse_loss(curr_Qs, target_Qs)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "Oj_zIimOfh8c"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "# Hyperparameters\n",
        "BUFFER_LIMIT = 2000\n",
        "\n",
        "MAX_EPISODES = 200\n",
        "BATCH_SIZE  = 32\n",
        "\n",
        "LEARNING_RATE = 0.001   # increase when cart keep moves one-way\n",
        "\n",
        "epsilon = 1.0\n",
        "MAX_EPSILON = 1.0                # Upper bound of epsilon\n",
        "MIN_EPSILON = 0.01               # Lower bound of epsilon\n",
        "DECAY_RATE = 0.005\n",
        "\n",
        "GAMMA = 0.99                       # Discount factor\n",
        "\n",
        "SEED = 1\n",
        "# ------------------------------------------------------- #\n",
        "env = gym.make(ENV_NAME)\n",
        "agent = DQNAgent(\n",
        "    env = env,\n",
        "    buffer_limit = BUFFER_LIMIT,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    gamma = GAMMA,\n",
        "    lr = LEARNING_RATE\n",
        ")\n",
        "\n",
        "# ------------------------------------ train loop ------------------------------------ #\n",
        "with tqdm(total = MAX_EPISODES, desc = \"Episode\") as pbar:\n",
        "  for episode in range(MAX_EPISODES):\n",
        "    score = []\n",
        "    ## Reset environment and get first new observation\n",
        "    state, _ = agent.env.reset(seed = SEED)\n",
        "    episode_reward = 0\n",
        "    done = False  # has the enviroment finished?\n",
        "\n",
        "    while not done:\n",
        "      action = agent.get_action(state, epsilon = epsilon)\n",
        "      next_state, reward, done, _, _ = agent.env.step(action)\n",
        "      agent.append_sample(state, action, reward, next_state, done)\n",
        "\n",
        "      state = next_state # update state\n",
        "      episode_reward += reward\n",
        "\n",
        "      # for visualization (not necessary)\n",
        "      if done:\n",
        "        scores.append(episode_reward)\n",
        "        pbar.set_postfix({'episode_reward': episode_reward})\n",
        "        pbar.update(1)\n",
        "        break\n",
        "\n",
        "      if len(agent.replay_buffer) >= agent.batch_size:\n",
        "        agent.train_step()\n",
        "\n",
        "    # epsilon -= (MAX_EPSILON - MIN_EPSILON) / MAX_EPISODES # Linear decay\n",
        "    epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON)*np.exp(-DECAY_RATE*episode)  # Exponential decay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWE7EO39ybK0",
        "outputId": "3e79f351-cf9d-4ffc-96fc-1181f7d58558"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode: 100%|██████████| 200/200 [00:48<00:00,  4.09it/s, episode_reward=155]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import Video\n",
        "import os\n",
        "\n",
        "def evaluate_dqn_model(env_name, model, num_episodes=5, device=\"cpu\"):\n",
        "\n",
        "    video_folder = \"./videos/\"\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    # Gymnasium 환경 생성\n",
        "    env = RecordVideo(gym.make(env_name, render_mode=\"rgb_array\"), video_folder=video_folder, episode_trigger=lambda x: True)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()  # 환경 초기화\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        print(f\"Episode {episode + 1}\")\n",
        "        while not done:\n",
        "\n",
        "            # 행동 선택 (탐욕 정책)\n",
        "            with torch.no_grad():\n",
        "                action = model.get_action(state, epsilon = epsilon)\n",
        "\n",
        "            # 환경에서 행동 수행\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            # 보상 합산\n",
        "            total_reward += reward\n",
        "\n",
        "            # 다음 상태로 이동\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Total Reward in Episode {episode + 1}: {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    video_path = os.path.join(video_folder, os.listdir(video_folder)[0])\n",
        "    return video_path\n",
        "\n",
        "# 모델 추론 실행\n",
        "if __name__ == \"__main__\":\n",
        "    ENV_NAME = \"CartPole-v1\"\n",
        "    # MODEL_PATH = \"dqn_cartpole.pth\"  # 학습된 모델 경로\n",
        "    video_path = evaluate_dqn_model(ENV_NAME, agent, num_episodes=5, device=\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "r9wn5Fn9RM_n",
        "outputId": "77829cdc-ae57-4224-b7c3-1cc23c893619"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1\n",
            "Total Reward in Episode 1: 117.0\n",
            "Episode 2\n",
            "Total Reward in Episode 2: 9.0\n",
            "Episode 3\n",
            "Total Reward in Episode 3: 182.0\n",
            "Episode 4\n",
            "Total Reward in Episode 4: 199.0\n",
            "Episode 5\n",
            "Total Reward in Episode 5: 194.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Video' object has no attribute 'display'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-513cd257a464>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_dqn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Video' object has no attribute 'display'"
          ]
        }
      ]
    }
  ]
}
